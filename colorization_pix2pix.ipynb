{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional pix2pix GAN\n",
    "### Adapted from [ericklindernoren](https://github.com/eriklindernoren/Keras-GAN/blob/master/pix2pix/pix2pix.py)\n",
    "### [pix2pix paper](https://arxiv.org/pdf/1611.07004.pdf)\n",
    "\n",
    "## TODO:\n",
    "1. Set up a github repository.\n",
    "2. Write a log file for each run that includes the date, name of experiment, details of parameter values, and metrics after each epoch (loss, time, etc.). This is particularly important since it is apparently impossible to see output from a Jupyter notebook if connection to the server is ever lost while it's running.\n",
    "3. Output should be both displayed in the notebook and saved to a folder and should include:\n",
    "  - Loss curves (should get overwritten in the output folder after each epoch, so just one image)\n",
    "  - Example outputs: for several training images, show several generated images (different noise inputs). Also show generated examples for test set examples to give a feel for generalization/overfitting.\n",
    "4. Make a new synthetic dataset with random number of circles (say 1-5) per image, each of which is a random color (choose from among several color options). Use LAB colorspace to generate the circles to ensure that all circles have the same L value before converting to RGB to save.\n",
    "5. Consider if a different \"real\" data set (e.g., hotel interiors) would be better than the current set of color images to show multimodality.\n",
    "6. Prediction: the GAN will learn to ignore noise input, so that generated images look similar for a given conditional input (L channel) even with different noise.\n",
    "7. Try to increase diversity by generating a set number of possible outputs (i.e., more channels); check out this paper: [Photographic Image Synthesis with Cascaded Refinement Networks](https://arxiv.org/pdf/1707.09405.pdf).\n",
    "8. Think about evaluation metrics, user study?\n",
    "9. Potential final comparisons for paper:\n",
    "  - U-NET with L2 loss (i.e., just the generator with no GAN, no noise)\n",
    "  - GAN with no noise\n",
    "  - GAN with noise\n",
    "  - GAN with diversity improvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "def sec_to_time(seconds):\n",
    "    days = seconds // (60 * 60 * 24)\n",
    "    seconds -= days * (60 * 60 * 24)\n",
    "    hours = seconds // (60 * 60)\n",
    "    seconds -= hours * (60 * 60)\n",
    "    minutes = seconds // 60\n",
    "    seconds -= minutes * 60\n",
    "    return f\"{int(days)}:{int(hours):02}:{int(minutes):02}:{int(seconds):02}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Reshape, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "import cv2\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_image_files(directory):\n",
    "    files = sorted(os.listdir(directory))\n",
    "    return [os.path.join(directory, f) for f in files if is_an_image_file(f)]\n",
    "\n",
    "def is_an_image_file(filename):\n",
    "    IMAGE_EXTENSIONS = ['.png', '.jpg', '.jpeg']\n",
    "    for ext in IMAGE_EXTENSIONS:\n",
    "        if ext in filename:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = cv2.imread(path[0])\n",
    "    \n",
    "    # Make sure all images are 256 x 256 by cropping them\n",
    "    r, c = img.shape[:2]\n",
    "    r_diff = (r - 256) // 2\n",
    "    c_diff = (c - 256) // 2\n",
    "    cropped = img[r_diff:256 + r_diff, c_diff:256 + c_diff] \n",
    "    return cropped\n",
    "\n",
    "def load_images(path, n_images):\n",
    "    if n_images < 0:\n",
    "        n_images = float(\"inf\")\n",
    "    all_image_paths = list_image_files(path)\n",
    "    images_l, images_ab = [], []\n",
    "    \n",
    "    # Initialize a progress bar with max of n_images\n",
    "    pbar = tqdm_notebook(total = n_images, desc=\"Loading Images...\")\n",
    "    \n",
    "    for path in zip(all_image_paths):\n",
    "        img = load_image(path)\n",
    "        lab_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "        lab_img = preprocess_image(lab_img)\n",
    "        \n",
    "        l = lab_img[:,:,0]\n",
    "        l = l[:,:,np.newaxis]\n",
    "        # Include all 3 channels, overwrite 1st channel with 0's\n",
    "        ab = lab_img[:,:,1:]\n",
    "\n",
    "        images_l.append(l)\n",
    "        images_ab.append(ab)\n",
    "\n",
    "        images_loaded = len(images_l)\n",
    "        \n",
    "        # Increase progress by one\n",
    "        pbar.update(1)\n",
    "        \n",
    "        if images_loaded > n_images - 1: \n",
    "            break\n",
    "\n",
    "    return {\n",
    "        'l': np.array(images_l),\n",
    "        'ab': np.array(images_ab)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESHAPE = (256,256)\n",
    "\n",
    "def preprocess_image(cv_img):\n",
    "    img = (cv_img - 127.5) / 127.5\n",
    "    return img\n",
    "\n",
    "def deprocess_image(img):\n",
    "    img = (img * 127.5) + 127.5\n",
    "    return img.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(np_arr, path):\n",
    "    img = np_arr * 127.5 + 127.5\n",
    "    im = Image.fromarray(img)\n",
    "    im.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(truth, gray, predicted, n_to_sample, shouldSave, sample_title):\n",
    "    predicted = predicted.astype(np.float64)\n",
    "    \n",
    "    grays_merged = []\n",
    "    truth_merged = []\n",
    "    predicted_merged = []\n",
    "    for i in range(n_to_sample):\n",
    "#         newImageIndex = i * 2\n",
    "        grays_merged.append(cv2.merge((gray[i], gray[i], gray[i])))\n",
    "        truth_merged.append(cv2.merge((gray[i], truth[i])))\n",
    "        predicted_merged.append(cv2.merge((gray[i], predicted[i])))\n",
    "    \n",
    "    r = truth.shape[1]\n",
    "    c = truth.shape[2]\n",
    "\n",
    "    figure = np.zeros([r * n_to_sample + 15 * (n_to_sample-1), c * 3 + 5 * (3-1), 3], dtype=np.uint8)\n",
    "    figure += 255\n",
    "    start_r = 0\n",
    "    \n",
    "    for i in range(n_to_sample):        \n",
    "        figure[start_r:start_r + r, :c] = cv2.cvtColor(deprocess_image(truth_merged[i]), cv2.COLOR_LAB2RGB)\n",
    "        figure[start_r:start_r + r, c + 5:2*(c)+5] = deprocess_image(grays_merged[i])\n",
    "        figure[start_r:start_r + r, 2*(c+5):3*c + 5*2] = cv2.cvtColor(deprocess_image(predicted_merged[i]), cv2.COLOR_LAB2RGB)\n",
    "        start_r += r + 15\n",
    "    img = Image.fromarray(figure, \"RGB\")\n",
    "    \n",
    "    dpi = plt.rcParams['figure.dpi']\n",
    "    height, width, _ = figure.shape\n",
    "\n",
    "    preview = plt.figure(figsize=(6,4))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(sample_title, fontsize = 'large')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if shouldSave:\n",
    "        # What size does the figure need to be in inches to fit the image?\n",
    "        figsize = width / float(dpi), height / float(dpi)\n",
    "        \n",
    "        full = plt.figure(figsize = figsize)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(sample_title, fontsize = 25)\n",
    "        plt.savefig(save_path + sample_title + \".png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_imgs_new(truth, gray, predictions, n_test_batch, shouldSave, sample_title=None):    \n",
    "    # Store all merged\n",
    "    merged_predictions = []\n",
    "    merged_grays = []\n",
    "    merged_truths = []\n",
    "        \n",
    "    # For each image...\n",
    "    for i in range(n_test_batch):\n",
    "        # Get prediction, merge\n",
    "        merged = []\n",
    "        # For each k...\n",
    "        for j in range(k):\n",
    "            # Generate a prediction...\n",
    "            prediction = predictions[i,:,:,2*j:(2*j)+2].astype(np.float64)\n",
    "            # And merge it to be a 3 channel image\n",
    "            merged.append(cv2.merge((gray[i], prediction)))\n",
    "        merged_predictions.append(merged) \n",
    "        \n",
    "        # Merge the grayscale channels\n",
    "        merged_grays.append(cv2.merge((gray[i], gray[i], gray[i])))\n",
    "        \n",
    "        # Merge the l and ab to create truth\n",
    "        merged_truths.append(cv2.merge((gray[i], truth[i])))\n",
    "\n",
    "    \n",
    "    r = truth.shape[1]\n",
    "    c = truth.shape[2]\n",
    "    \n",
    "    start_r = 0\n",
    "\n",
    "    n_predictions = k\n",
    "    # Create figure\n",
    "    columns = 2 + n_predictions\n",
    "    figure = np.zeros([r * n_test_batch + (15 * (n_test_batch-1)), (c * columns) + 5 * (columns-1), 3], dtype=np.uint8)\n",
    "    figure += 255\n",
    "    \n",
    "    # Loop through sets of test images\n",
    "    for i in range(n_test_batch):\n",
    "        # Place truth and gray\n",
    "        figure[start_r:start_r + r, :c] = cv2.cvtColor(deprocess_image(merged_truths[i]), cv2.COLOR_LAB2RGB)\n",
    "        figure[start_r:start_r + r, c + 5:2*c + 5] = deprocess_image(merged_grays[i])\n",
    "\n",
    "        # Place multiple preditions in figure\n",
    "        for j in range(n_predictions):  \n",
    "            current_predictions = merged_predictions[i]\n",
    "            figure[start_r:start_r + r, (2+j)*(c+5):(3+j)*c + (5*(2+j))] = \\\n",
    "                cv2.cvtColor(deprocess_image(current_predictions[j]), cv2.COLOR_LAB2RGB)\n",
    "        start_r += r + 15\n",
    "    img = Image.fromarray(figure, \"RGB\")\n",
    "\n",
    "    dpi = plt.rcParams['figure.dpi']\n",
    "    height, width, _ = figure.shape\n",
    "\n",
    "    preview = plt.figure(figsize=(6,4))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    title = sample_title\n",
    "    plt.title(title, fontsize = 'large')\n",
    "    plt.show()\n",
    "\n",
    "    if shouldSave:\n",
    "        # What size does the figure need to be in inches to fit the image?\n",
    "        figsize = width / float(dpi), height / float(dpi)\n",
    "\n",
    "        full = plt.figure(figsize = figsize)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(title, fontsize = 25)\n",
    "        plt.savefig(save_path + sample_title + \".png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(H, W, k):\n",
    "    # Inputs: height and width of the input image\n",
    "    # Returns the model, which generates the AB channels\n",
    "\n",
    "    # Pix2pix adapted from \n",
    "    # https://github.com/eriklindernoren/Keras-GAN/blob/master/pix2pix/pix2pix.py\n",
    "\n",
    "    def conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "        \"\"\"Layers used during downsampling\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if bn:\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "        return d\n",
    "\n",
    "    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "        \"\"\"Layers used during upsampling\"\"\"\n",
    "        u = UpSampling2D(size=2)(layer_input)\n",
    "        u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "        if dropout_rate:\n",
    "            u = Dropout(dropout_rate)(u)\n",
    "        u = BatchNormalization(momentum=0.8)(u)\n",
    "        u = Concatenate()([u, skip_input])\n",
    "        return u\n",
    "\n",
    "    gf = 64 # Number of filters in the first layer of G\n",
    "\n",
    "    noise_in = Input(shape=(100,))\n",
    "    condition_in = Input(shape=(H, W, 1))\n",
    "    \n",
    "    # pass noise through a FC layer to get it to the right size\n",
    "    noise = Dense(H * H)(noise_in)\n",
    "\n",
    "    # reshape to be the size of an image channel\n",
    "    noise = Reshape((H, H, 1))(noise)\n",
    "    \n",
    "    # stick the (somewhat modified) noise as the second channel after\n",
    "    # the gray input. Assuming new dimension of hid will be\n",
    "    # B x 256 x 256 x 2, where B is the batch size.\n",
    "#     d0 = Concatenate(axis=-1)([condition_in, noise])\n",
    "    d0 = condition_in # Don't need noise since it's being ignored anyway\n",
    "\n",
    "    # U-NET\n",
    "    # Downsampling\n",
    "    d1 = conv2d(d0, gf, bn=False)\n",
    "    d2 = conv2d(d1, gf*2)\n",
    "    d3 = conv2d(d2, gf*4)\n",
    "    d4 = conv2d(d3, gf*8)\n",
    "    d5 = conv2d(d4, gf*8)\n",
    "    d6 = conv2d(d5, gf*8)\n",
    "    d7 = conv2d(d6, gf*8)\n",
    "\n",
    "    # Upsampling\n",
    "    u1 = deconv2d(d7, d6, gf*8)\n",
    "    u2 = deconv2d(u1, d5, gf*8)\n",
    "    u3 = deconv2d(u2, d4, gf*8)\n",
    "    u4 = deconv2d(u3, d3, gf*4)\n",
    "    u5 = deconv2d(u4, d2, gf*2)\n",
    "    u6 = deconv2d(u5, d1, gf)\n",
    "\n",
    "    u7 = UpSampling2D(size=2)(u6)\n",
    "    \n",
    "    # Final 2-channel AB image with values between -1 and 1\n",
    "    img_out = Conv2D(2*k, kernel_size=4, strides=1, padding='same', activation='tanh', name='pred_ab')(u7)\n",
    "\n",
    "    # Make Model\n",
    "    model = Model(inputs=[noise_in, condition_in], outputs=img_out)\n",
    "    \n",
    "    # Show summary of layers\n",
    "    print(\"Generator Model:\")\n",
    "    model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator(H, W, k):\n",
    "    # Inputs: height and width of the input image\n",
    "    # Returns the model, which predicts real/fake\n",
    "    # over a set of spatial regions (i.e., predicts a matrix instead of a scalar).\n",
    "\n",
    "    # Pix2pix adapted from \n",
    "    # https://github.com/eriklindernoren/Keras-GAN/blob/master/pix2pix/pix2pix.py\n",
    "\n",
    "    def d_layer(layer_input, filters, f_size=4, bn=True):\n",
    "        \"\"\"Discriminator layer\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if bn:\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "        return d\n",
    "\n",
    "    # Number of filters in the first layer of D\n",
    "    df = 64\n",
    "\n",
    "    img_in = Input(shape=(H, W, 2*k)) # AB channels\n",
    "    condition_in = Input(shape=(H, W, 1)) # L channel\n",
    "    \n",
    "    # Concat the L and AB channels\n",
    "    concat_imgs = Concatenate()([condition_in, img_in])\n",
    "\n",
    "    d1 = d_layer(concat_imgs, df, bn=False)\n",
    "    d2 = d_layer(d1, df*2)\n",
    "    d3 = d_layer(d2, df*4)\n",
    "    d4 = d_layer(d3, df*8)\n",
    "\n",
    "    # validity map is a one-channel matrix 1/16 the size of the input (halved 4 times).\n",
    "    # Each number predicts whether a region of the input is real/fake.\n",
    "    validity = Conv2D(1*k, kernel_size=4, strides=1, padding='same', name='pred_valid')(d4)\n",
    "\n",
    "    # Build Model\n",
    "    model = Model(inputs=[img_in, condition_in], outputs=validity)\n",
    "\n",
    "    # Show summary of layers\n",
    "    print(\"Disciminator Model:\")\n",
    "    model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_k_gen_loss(y_true, y_pred):\n",
    "    y_true = K.reshape(y_true, (-1, H, W, k, 2))\n",
    "    y_pred = K.reshape(y_pred, (-1, H, W, k, 2))\n",
    "\n",
    "    print(\"true:\", y_true.shape)\n",
    "    print(\"pred:\", y_pred.shape)\n",
    "\n",
    "    diff = y_true - y_pred\n",
    "    diff = K.abs(diff)\n",
    "    diff = K.mean(diff, axis=(1, 2, 4))\n",
    "    print(\"current size:\", diff.shape)\n",
    "    min_for_each_n = K.min(diff, axis=1)\n",
    "    print(\"current size:\", min_for_each_n.shape)\n",
    "    return K.sum(min_for_each_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "def generate_noise(n_samples, noise_dim):\n",
    "    X = np.random.normal(0, 1, size=(n_samples, noise_dim))\n",
    "    return X\n",
    "\n",
    "def show_samples(batchidx, save_path):\n",
    "    fig, axs = plt.subplots(5, 6, figsize=(10,6))\n",
    "    plt.subplots_adjust(hspace=0.3, wspace=0.1)\n",
    "    #fig, axs = plt.subplots(5, 6)\n",
    "    #fig.tight_layout()\n",
    "    for classlabel in range(10):\n",
    "        row = int(classlabel / 2)\n",
    "        coloffset = (classlabel % 2) * 3\n",
    "        lbls = one_hot_encode([classlabel] * 3)\n",
    "        noise = generate_noise(3, 100)\n",
    "        gen_imgs = generator.predict([noise, lbls])\n",
    "\n",
    "        for i in range(3):\n",
    "            # Dont scale the images back, let keras handle it\n",
    "            img = image.array_to_img(gen_imgs[i], scale=True)\n",
    "            axs[row,i+coloffset].imshow(img)\n",
    "            axs[row,i+coloffset].axis('off')\n",
    "            if i ==1:\n",
    "                axs[row,i+coloffset].set_title(tags[classlabel])\n",
    "    plt.savefig(save_path + '.jpg')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(g_losses, d_losses, epoch_num):\n",
    "    # Scale axis so it lines up with epoch_num\n",
    "    x_axis = [epoch_num * x / (len(g_losses) - 1) for x in range(len(g_losses))]\n",
    "    \n",
    "    ax = subplot(1,1,1)\n",
    "    ax.plot(x_axis, g_losses, label=\"Generator\")\n",
    "    ax.plot(x_axis, d_losses, label=\"Discriminator\")\n",
    "    plt.title(\"Losses After \" + str(epoch_num) + \" Epochs\")\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, labels)\n",
    "    plt.savefig(save_path + \"Losses.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"pix2pix_multimodal_lsun_5000_imgs_k_10\"\n",
    "\n",
    "n_images = 5000    # Number of images to load for training; recommended = 512\n",
    "BATCH_SIZE = 6\n",
    "N_EPOCHS = 100\n",
    "\n",
    "# dataset = 'new_circles/'\n",
    "dataset = 'lsun/'\n",
    "\n",
    "k = 10 # Number of modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to store output\n",
    "generic_output_folder = \"Output/\"\n",
    "new_output_folder = test_name + \"/\"\n",
    "save_path = generic_output_folder + new_output_folder\n",
    "\n",
    "# Ensure output can save in desired location\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84760b0699c941fe89094fe438c53ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading Images...', max=5000, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get training images\n",
    "# Load dataset, convert to LAB, normalize to range [-1, 1]\n",
    "data = load_images(dataset + 'train', n_images)\n",
    "\n",
    "# Separate into inputs (X) and outputs (Y)\n",
    "y_train, X_train = data['ab'], data['l']\n",
    "\n",
    "num_batches = X_train.shape[0] // BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images(y_train, X_train, y_train, 3, False, \"Proof that images are read correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN creation\n",
    "H = W = 256\n",
    "\n",
    "# Discriminator loss - MSE seems to produce better results\n",
    "#discrim_loss = 'binary_crossentropy'\n",
    "discrim_loss = 'mse'\n",
    "\n",
    "# 1. Discriminator\n",
    "# Calculate output shape of D (PatchGAN)\n",
    "patch = H // 2**4 # Input size gets cut in half 4 times\n",
    "discriminator = get_discriminator(H, W, k)\n",
    "discriminator.name = 'discrim_model' # Need a name for the loss dictionary below\n",
    "discriminator.compile(optimizer=Adam(2e-4, 0.5), loss=discrim_loss, metrics=['accuracy'])\n",
    "discriminator.trainable = False # For the combined model we will only train the generator\n",
    "print(\"\\n\")\n",
    "\n",
    "# 2. Generator\n",
    "generator = get_generator(H, W, k)\n",
    "generator.name = 'gen_model' # Need a name for the loss dictionary below\n",
    "\n",
    "# 3. GAN\n",
    "gan_noise_in = Input(shape=(100,))\n",
    "gan_condition_in = Input(shape=(H, W, 1))\n",
    "\n",
    "# By conditioning on L generate a fake version of AB\n",
    "fake_AB = generator([gan_noise_in, gan_condition_in])\n",
    "\n",
    "# Discriminator determines validity of AB images / L pairs\n",
    "print(\"fake_ab:\", fake_AB.shape)\n",
    "\n",
    "print(\"gan_condition_in:\", gan_condition_in.shape)\n",
    "\n",
    "valid = discriminator([fake_AB, gan_condition_in])\n",
    "\n",
    "losses = {'gen_model': min_k_gen_loss, # used to be 'mae'\n",
    "          'discrim_model': discrim_loss}\n",
    "loss_weights = {'gen_model': 100.0, 'discrim_model': 1.0}\n",
    "\n",
    "gan = Model(inputs=[gan_noise_in, gan_condition_in], outputs=[fake_AB, valid])\n",
    "gan.compile(optimizer=Adam(2e-4, 0.5), loss=losses, loss_weights=loss_weights)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "d_accuracies = []\n",
    "\n",
    "# PatchGAN loss ground truths\n",
    "true_labels = np.ones((BATCH_SIZE, patch, patch, k))\n",
    "fake_labels = np.zeros((BATCH_SIZE, patch, patch, k))\n",
    "\n",
    "for epoch in tqdm_notebook(range(N_EPOCHS), desc=\"Training GAN...\"):\n",
    "\n",
    "    cum_d_loss = 0.\n",
    "    cum_g_loss = 0.\n",
    "    cum_d_acc = 0.\n",
    "\n",
    "    pbar_message = \"Epoch \" + str(epoch + 1)\n",
    "    for batch_idx in tqdm_notebook(range(num_batches), desc=pbar_message):\n",
    "        # Random indices for training examples\n",
    "        idx = randint(0, y_train.shape[0], BATCH_SIZE)\n",
    "\n",
    "        # X is L channel, Y is AB\n",
    "        # Get the next set of real images to be used in this iteration\n",
    "        ab_channel = y_train[idx]\n",
    "        ab_vis = ab_channel.copy()\n",
    "        \n",
    "        ab_channel = np.expand_dims(ab_channel, axis=3)\n",
    "        ab_channel = np.repeat(ab_channel, k, axis=3) # repeat along the newly created k axis\n",
    "        ab_channel = np.reshape(ab_channel, (-1, H, W, 2*k)) # stack the channels so things are the right shape\n",
    "        \n",
    "        # Get the next set of labels (grayscale images to be used in this iteration)\n",
    "        l_channel = X_train[idx]\n",
    "\n",
    "        noise_data = generate_noise(BATCH_SIZE, 100)\n",
    "\n",
    "        # Generate fake AB channels from L\n",
    "        fake_ab = generator.predict([noise_data, l_channel])\n",
    "\n",
    "        # Train discriminator on fake and real data\n",
    "        d_loss_fake, d_acc_fake = discriminator.train_on_batch([fake_ab, l_channel], fake_labels)\n",
    "        d_loss_true, d_acc_true = discriminator.train_on_batch([ab_channel, l_channel], true_labels)\n",
    "        d_loss = (d_loss_fake + d_loss_true) / 2\n",
    "        d_acc = (d_acc_fake + d_acc_true) / 2\n",
    "        \n",
    "        cum_d_loss += d_loss\n",
    "        cum_d_acc += d_acc\n",
    "\n",
    "        # Train generator (by way of the whole gan)\n",
    "        # Note that there are 3 losses. The first should be the\n",
    "        # weighted sum of the adversarial and L1 loss values for the batch.\n",
    "        # Ref: https://machinelearningmastery.com/how-to-implement-pix2pix-gan-models-from-scratch-with-keras/\n",
    "        g_loss, _, _ = gan.train_on_batch([noise_data, l_channel], [ab_channel, true_labels])\n",
    "        cum_g_loss += g_loss\n",
    "\n",
    "        g_losses.append(g_loss)\n",
    "        d_losses.append(d_loss)\n",
    "        d_accuracies.append(d_acc)\n",
    "\n",
    "    print('Epoch: {:04d}, Gen Loss: {:0.4f}, Discrim Loss: {:0.4f}, Discrim Acc: {:0.4f}'.format(\n",
    "        epoch+1, cum_g_loss/num_batches, cum_d_loss/num_batches, cum_d_acc/num_batches))\n",
    "    sample_title = \"Epoch \" + str(epoch+1)\n",
    "    sample_imgs_new(ab_vis, l_channel, fake_ab, 3, True, sample_title)\n",
    "\n",
    "    plot_losses(g_losses, d_losses, epoch + 1)\n",
    "\n",
    "gan.save_weights(save_path +'GAN_Weights.h5')\n",
    "discriminator.save_weights(save_path + 'Discriminator_Weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's see how the predictions look on test images..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator(generator, truth, gray, n_test_batch, n_predictions, shouldSave, sample_title=None):\n",
    "    # Each test image will use the same random noise to show various predictions\n",
    "    noise_arrays = generate_noise(n_test_batch, 100)\n",
    "    \n",
    "    # Store all merged\n",
    "    merged_predictions = []\n",
    "    merged_grays = []\n",
    "    merged_truths = []\n",
    "    \n",
    "    predictions = generator.predict([noise_arrays, gray[:n_test_batch]])\n",
    "    \n",
    "    # For each image...\n",
    "    for i in range(n_test_batch):\n",
    "        # Get prediction, merge\n",
    "        merged = []\n",
    "        # For each k...\n",
    "        for j in range(k):\n",
    "            # Generate a prediction...\n",
    "            prediction = predictions[i,:,:,2*j:(2*j)+1].astype(np.float64)\n",
    "            # And merge it to be a 3 channel image\n",
    "            merged.append(cv2.merge((gray[i], prediction)))\n",
    "        merged_predictions.append(merged) \n",
    "        \n",
    "        # Merge the grayscale channels\n",
    "        merged_grays.append(cv2.merge((gray[i], gray[i], gray[i])))\n",
    "        \n",
    "        # Merge the l and ab to create truth\n",
    "        merged_truths.append(cv2.merge((gray[i], truth[i])))\n",
    "\n",
    "    \n",
    "    r = truth.shape[1]\n",
    "    c = truth.shape[2]\n",
    "    \n",
    "    # Loop through sets of test images\n",
    "    for i in range(len(truth)//n_test_batch):\n",
    "        start = 3*i\n",
    "        end = start + 3\n",
    "        \n",
    "        # Create figure\n",
    "        columns = 2 + n_predictions\n",
    "        figure = np.zeros([r * n_test_batch + (15 * (n_test_batch-1)), (c * columns) + 5 * (columns-1), 3], dtype=np.uint8)\n",
    "        figure += 255\n",
    "        start_r = 0\n",
    "\n",
    "        # Place images in figure\n",
    "        for j in range(n_test_batch):  \n",
    "            figure[start_r:start_r + r, :c] = cv2.cvtColor(deprocess_image(merged_truths[start + j]), cv2.COLOR_LAB2RGB)\n",
    "            figure[start_r:start_r + r, c + 5:2*c + 5] = deprocess_image(merged_grays[start + j])\n",
    "            # Loop through and place multiple predictions\n",
    "            for pred_index in range(n_predictions):\n",
    "                current_predictions = merged_predictions[start + j]\n",
    "                figure[start_r:start_r + r, (2+pred_index)*(c+5):(3+pred_index)*c + (5*(2+pred_index))] = \\\n",
    "                    cv2.cvtColor(deprocess_image(current_predictions[pred_index]), cv2.COLOR_LAB2RGB)\n",
    "            start_r += r + 15\n",
    "        img = Image.fromarray(figure, \"RGB\")\n",
    "\n",
    "        dpi = plt.rcParams['figure.dpi']\n",
    "        height, width, _ = figure.shape\n",
    "\n",
    "        preview = plt.figure(figsize=(6,4))\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        if sample_title != None:\n",
    "            title = sample_title + \" \" + str(i+1)\n",
    "            plt.title(title, fontsize = 'large')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        if shouldSave:\n",
    "            # What size does the figure need to be in inches to fit the image?\n",
    "            figsize = width / float(dpi), height / float(dpi)\n",
    "\n",
    "            full = plt.figure(figsize = figsize)\n",
    "            plt.imshow(img)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(title, fontsize = 25)\n",
    "            if sample_title != None:\n",
    "                plt.savefig(save_path + sample_title + \" \" + str(i+1) + \".png\")\n",
    "            else:\n",
    "                plt.savefig(save_path + \"Test Images \" + str(i+1) + \".png\")\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in some test images\n",
    "num_test_imgs = 15\n",
    "data = load_images(dataset + 'test', num_test_imgs)\n",
    "\n",
    "# Separate into inputs (X) and outputs (Y)\n",
    "y_test, X_test = data['ab'], data['l']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "noise = generate_noise(num_test_imgs, 100)\n",
    "predictions = generator.predict([noise, X_test])\n",
    "\n",
    "num_to_display = 3\n",
    "for i in range(num_test_imgs // num_to_display):\n",
    "    start = num_to_display * i\n",
    "    end = start + num_to_display\n",
    "    \n",
    "    truth_ab = y_test[start:end]\n",
    "    grays = X_test[start:end]\n",
    "    predicted_ab = predictions[start:end]\n",
    "    \n",
    "    title = \"Test Images \" + str(i+1)\n",
    "    sample_imgs_new(truth_ab, grays, predicted_ab, num_to_display, True, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find how often each K was the min loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_loss(y_true, y_pred):\n",
    "    y_true = np.reshape(y_true, (-1, H, W, k, 2))\n",
    "    y_pred = np.reshape(y_pred, (-1, H, W, k, 2))\n",
    "\n",
    "    diff = y_true - y_pred\n",
    "    diff = np.abs(diff)\n",
    "    diff = np.mean(diff, axis=(1, 2, 4))\n",
    "    min_for_each_n = np.min(diff, axis=1)\n",
    "    min_k = np.argmin(diff, axis=1).item()\n",
    "\n",
    "    return np.sum(min_for_each_n), min_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = generate_noise(n_images, 100)\n",
    "predictions = generator.predict([noise, X_train])\n",
    "\n",
    "losses = []\n",
    "k_chosen = {}\n",
    "for i, prediction in enumerate(predictions):\n",
    "    truth_ab = y_train[i]\n",
    "    truth_ab = np.expand_dims(truth_ab, axis=0)\n",
    "    truth_ab = np.expand_dims(truth_ab, axis=3)\n",
    "    truth_ab = np.repeat(truth_ab, k, axis=3) # repeat along the newly created k axis\n",
    "    truth_ab = np.reshape(truth_ab, (-1, H, W, 2*k)) # stack the channels so things are the right shape\n",
    "    \n",
    "    loss, curr_k = np_loss(truth_ab, prediction)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if curr_k not in k_chosen:\n",
    "        k_chosen[curr_k] = 1\n",
    "    else:\n",
    "        k_chosen[curr_k] += 1\n",
    "    \n",
    "avg_loss = sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write a log file with all the params so it's easier to remember what each run was\n",
    "print(\"Writing log file...\")\n",
    "filename = save_path + \"log.txt\"\n",
    "log_file = open(filename, \"w+\")\n",
    "log_file.write(\"Test Name: \" + test_name + \"\\n\\n\")\n",
    "\n",
    "log_file.write(\"Number of Images: \" + str(n_images) + \"\\n\")\n",
    "log_file.write(\"Number of Epochs: \" + str(N_EPOCHS) + \"\\n\\n\")\n",
    "\n",
    "log_file.write(\"Elapsed Time: \" + sec_to_time(time_end-time_start) + \"\\n\\n\")\n",
    "\n",
    "log_file.write(\"Start Date: \" + datetime.datetime.fromtimestamp(time_start).strftime(\"%b %d, %Y %I:%M:%S %p\") + \"\\n\")\n",
    "log_file.write(\"End Date: \" + datetime.datetime.fromtimestamp(time_end).strftime(\"%b %d, %Y %I:%M:%S %p\\n\\n\"))\n",
    "\n",
    "log_file.write(\"K: \" + str(k) + \"\\n\")\n",
    "log_file.write(\"Avg loss: \" + str(avg_loss) + \"\\n\")\n",
    "log_file.write(\"Times K has been chosen: \")\n",
    "\n",
    "percents = [value / sum(list(map(int, k_chosen.values()))) for value in k_chosen.values()]\n",
    "for percent in percents:\n",
    "    log_file.write(str(percent) + \" \")\n",
    "log_file.close()\n",
    "print(\"Saved to \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(k), percents)\n",
    "plt.title(\"Frequency of Each K Being the Minimized Loss\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"% Chosen\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
