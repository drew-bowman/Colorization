{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colorization Test Model\n",
    "## This program will load an already-trained model, run it against a test set, and save individual test images\n",
    "## Output will be saved in ./Test_Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Reshape, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.engine.saving import load_model\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_image_files(directory):\n",
    "    files = sorted(os.listdir(directory))\n",
    "    return [os.path.join(directory, f) for f in files if is_an_image_file(f)]\n",
    "\n",
    "def is_an_image_file(filename):\n",
    "    IMAGE_EXTENSIONS = ['.png', '.jpg', '.jpeg']\n",
    "    for ext in IMAGE_EXTENSIONS:\n",
    "        if ext in filename:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = cv2.imread(path[0])\n",
    "    \n",
    "    # Make sure all images are 256 x 256 by cropping them\n",
    "    r, c = img.shape[:2]\n",
    "    r_diff = (r - 256) // 2\n",
    "    c_diff = (c - 256) // 2\n",
    "    cropped = img[r_diff:256 + r_diff, c_diff:256 + c_diff] \n",
    "    return cropped\n",
    "\n",
    "def load_images(path, n_images=-1, shouldShuffle=False):\n",
    "    all_image_paths = list_image_files(path)\n",
    "    if shouldShuffle:\n",
    "        random.shuffle(all_image_paths)\n",
    "    \n",
    "    if n_images < 0:\n",
    "        n_images = len(all_image_paths)\n",
    "    images_l, images_ab = [], []\n",
    "    \n",
    "    # Initialize a progress bar with max of n_images\n",
    "    pbar = tqdm_notebook(total = n_images, desc=\"Loading Images...\")\n",
    "    \n",
    "    for path in zip(all_image_paths):\n",
    "        img = load_image(path)\n",
    "        lab_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "        lab_img = preprocess_image(lab_img)\n",
    "        \n",
    "        l = lab_img[:,:,0]\n",
    "        l = l[:,:,np.newaxis]\n",
    "        # Include all 3 channels, overwrite 1st channel with 0's\n",
    "        ab = lab_img[:,:,1:]\n",
    "\n",
    "        images_l.append(l)\n",
    "        images_ab.append(ab)\n",
    "\n",
    "        images_loaded = len(images_l)\n",
    "        \n",
    "        # Increase progress by one\n",
    "        pbar.update(1)\n",
    "        \n",
    "        if images_loaded > n_images - 1: \n",
    "            break\n",
    "\n",
    "    return {\n",
    "        'l': np.array(images_l),\n",
    "        'ab': np.array(images_ab)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESHAPE = (256,256)\n",
    "\n",
    "def preprocess_image(cv_img):\n",
    "    img = (cv_img - 127.5) / 127.5\n",
    "    return img\n",
    "\n",
    "def deprocess_image(img):\n",
    "    img = (img * 127.5) + 127.5\n",
    "    return img.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(np_arr, path):\n",
    "    img = np_arr * 127.5 + 127.5\n",
    "    im = Image.fromarray(img)\n",
    "    im.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(H, W, k):\n",
    "    # Inputs: height and width of the input image\n",
    "    # Returns the model, which generates the AB channels\n",
    "\n",
    "    # Pix2pix adapted from \n",
    "    # https://github.com/eriklindernoren/Keras-GAN/blob/master/pix2pix/pix2pix.py\n",
    "\n",
    "    def conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "        \"\"\"Layers used during downsampling\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if bn:\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "        return d\n",
    "\n",
    "    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "        \"\"\"Layers used during upsampling\"\"\"\n",
    "        u = UpSampling2D(size=2)(layer_input)\n",
    "        u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "        if dropout_rate:\n",
    "            u = Dropout(dropout_rate)(u)\n",
    "        u = BatchNormalization(momentum=0.8)(u)\n",
    "        u = Concatenate()([u, skip_input])\n",
    "        return u\n",
    "\n",
    "    gf = 64 # Number of filters in the first layer of G\n",
    "\n",
    "    noise_in = Input(shape=(100,))\n",
    "    condition_in = Input(shape=(H, W, 1))\n",
    "    \n",
    "    # pass noise through a FC layer to get it to the right size\n",
    "    noise = Dense(H * H)(noise_in)\n",
    "\n",
    "    # reshape to be the size of an image channel\n",
    "    noise = Reshape((H, H, 1))(noise)\n",
    "    \n",
    "    # stick the (somewhat modified) noise as the second channel after\n",
    "    # the gray input. Assuming new dimension of hid will be\n",
    "    # B x 256 x 256 x 2, where B is the batch size.\n",
    "    d0 = Concatenate(axis=-1)([condition_in, noise])\n",
    "#     d0 = condition_in # Don't need noise since it's being ignored anyway\n",
    "\n",
    "    # U-NET\n",
    "    # Downsampling\n",
    "    d1 = conv2d(d0, gf, bn=False)\n",
    "    d2 = conv2d(d1, gf*2)\n",
    "    d3 = conv2d(d2, gf*4)\n",
    "    d4 = conv2d(d3, gf*8)\n",
    "    d5 = conv2d(d4, gf*8)\n",
    "    d6 = conv2d(d5, gf*8)\n",
    "    d7 = conv2d(d6, gf*8)\n",
    "\n",
    "    # Upsampling\n",
    "    u1 = deconv2d(d7, d6, gf*8)\n",
    "    u2 = deconv2d(u1, d5, gf*8)\n",
    "    u3 = deconv2d(u2, d4, gf*8)\n",
    "    u4 = deconv2d(u3, d3, gf*4)\n",
    "    u5 = deconv2d(u4, d2, gf*2)\n",
    "    u6 = deconv2d(u5, d1, gf)\n",
    "\n",
    "    u7 = UpSampling2D(size=2)(u6)\n",
    "    \n",
    "    # Final 2-channel AB image with values between -1 and 1\n",
    "    img_out = Conv2D(2*k, kernel_size=4, strides=1, padding='same', activation='tanh', name='pred_ab')(u7)\n",
    "\n",
    "    # Make Model\n",
    "    model = Model(inputs=[noise_in, condition_in], outputs=img_out)\n",
    "    \n",
    "    # Show summary of layers\n",
    "    print(\"Generator Model:\")\n",
    "    model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator(H, W, k):\n",
    "    # Inputs: height and width of the input image\n",
    "    # Returns the model, which predicts real/fake\n",
    "    # over a set of spatial regions (i.e., predicts a matrix instead of a scalar).\n",
    "\n",
    "    # Pix2pix adapted from \n",
    "    # https://github.com/eriklindernoren/Keras-GAN/blob/master/pix2pix/pix2pix.py\n",
    "\n",
    "    def d_layer(layer_input, filters, f_size=4, bn=True):\n",
    "        \"\"\"Discriminator layer\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if bn:\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "        return d\n",
    "\n",
    "    # Number of filters in the first layer of D\n",
    "    df = 64\n",
    "\n",
    "    img_in = Input(shape=(H, W, 2*k)) # AB channels\n",
    "    condition_in = Input(shape=(H, W, 1)) # L channel\n",
    "    \n",
    "    # Concat the L and AB channels\n",
    "    concat_imgs = Concatenate()([condition_in, img_in])\n",
    "\n",
    "    d1 = d_layer(concat_imgs, df, bn=False)\n",
    "    d2 = d_layer(d1, df*2)\n",
    "    d3 = d_layer(d2, df*4)\n",
    "    d4 = d_layer(d3, df*8)\n",
    "\n",
    "    # validity map is a one-channel matrix 1/16 the size of the input (halved 4 times).\n",
    "    # Each number predicts whether a region of the input is real/fake.\n",
    "    validity = Conv2D(1*k, kernel_size=4, strides=1, padding='same', name='pred_valid')(d4)\n",
    "\n",
    "    # Build Model\n",
    "    model = Model(inputs=[img_in, condition_in], outputs=validity)\n",
    "\n",
    "    # Show summary of layers\n",
    "    print(\"Disciminator Model:\")\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_k_diff(y_true, y_pred):\n",
    "    # Shape: (Batch, H, W, k, 2)\n",
    "    y_true = K.reshape(y_true, (-1, H, W, k, 2))\n",
    "    y_pred = K.reshape(y_pred, (-1, H, W, k, 2))\n",
    "\n",
    "    print(\"true:\", y_true.shape)\n",
    "    print(\"pred:\", y_pred.shape)\n",
    "\n",
    "    diff = y_true - y_pred\n",
    "    diff = K.abs(diff)\n",
    "    diff = K.mean(diff, axis=(1, 2, 4)) # mean of (H, W, 2) leaves (B, k)\n",
    "    \n",
    "    loss_metric = diff\n",
    "\n",
    "    min_for_each_batch = K.min(loss_metric, axis=1)\n",
    "    return K.sum(min_for_each_batch) #* .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "def generate_noise(n_samples, noise_dim):\n",
    "    X = np.random.normal(0, 1, size=(n_samples, noise_dim))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_rgb_img(l, ab, filename):\n",
    "    # Make sure ab is the right type, generated imgs change to float32\n",
    "    ab = ab.astype(np.float64)\n",
    "    \n",
    "    # Merge\n",
    "    merged = cv2.merge((l, ab))\n",
    "    \n",
    "    # Get between 0, 255\n",
    "    deprocessed = deprocess_image(merged)\n",
    "    \n",
    "    # Change to BGR (Curse you CV2!!!)\n",
    "    rgb = cv2.cvtColor(deprocessed, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "    # Save\n",
    "    cv2.imwrite(save_path + filename, rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discrim_models():\n",
    "    # Load training data\n",
    "    max_imgs = 2500\n",
    "    if len(list_image_files(train_dataset)) > max_imgs:\n",
    "        num_imgs = max_imgs\n",
    "    else:\n",
    "        num_imgs = -1\n",
    "\n",
    "    train_data = load_images(train_dataset, num_imgs, True)\n",
    "    train_l, train_ab = train_data['l'], train_data['ab']\n",
    "\n",
    "    # Make generated data\n",
    "    noise = generate_noise(len(train_l), 100)\n",
    "    train_predictions = generator.predict([noise, train_l])\n",
    "\n",
    "    # Tile truth data\n",
    "    tiled = np.tile(train_ab, k)\n",
    "\n",
    "    # Make discrim predictions\n",
    "    generated_discrim_values = discriminator.predict([train_predictions, train_l])\n",
    "    true_discrim_values = discriminator.predict([tiled, train_l])\n",
    "    \n",
    "    # Flatten discrim values\n",
    "    n = generated_discrim_values.shape\n",
    "    flat_gen_discrim_val = generated_discrim_values.reshape((n[0], n[1] * n[2], n[3]))\n",
    "    flat_true_discrim_val = true_discrim_values.reshape((n[0], n[1] * n[2], n[3]))\n",
    "    \n",
    "    # Make labels\n",
    "    model_labels = np.concatenate((np.zeros(len(generated_discrim_values)), np.ones(len(true_discrim_values))))\n",
    "    \n",
    "    # Loop through and store models\n",
    "    models = []\n",
    "    for i in range(k):\n",
    "        model_data = np.concatenate((flat_gen_discrim_val[:,:,i], flat_true_discrim_val[:,:,i]))\n",
    "        model = LogisticRegression(max_iter=1000).fit(model_data, model_labels)\n",
    "        models.append(model)\n",
    "        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "\n",
    "# Program will grab 100 epoch weights for G,D in ./Output/trained_model_name/\n",
    "# trained_model_name = \"lsun_colorization_full_model\"\n",
    "# trained_model_name = \"places2_colorization_read_imgs_flow_test\"\n",
    "trained_model_name = \"places2_final_k_1_with_noise\"\n",
    "# trained_model_name = 'ablation_circles_equal_l_k_3_no_noise_no_aug'\n",
    "\n",
    "# Program will save output in ./TestOutput/trained_model_name by default\n",
    "# Change from none to save output in ./TestOutput/overwrite_save_dir\n",
    "# overwrite_save_dir = \"WHJ_Predictions\"\n",
    "# overwrite_save_dir = \"LSUN_on_places2\"\n",
    "overwrite_save_dir = None\n",
    "\n",
    "# Should output have the same file names as the test images?\n",
    "preserve_img_names = True\n",
    "\n",
    "# Must match k from model\n",
    "k = 1\n",
    "\n",
    "# Testing parameters\n",
    "num_test_imgs = 100\n",
    "\n",
    "# Should program randomly colorize some test images and leave some ground truth?\n",
    "# This was used to generate data for a user study\n",
    "random_select_gt_or_colorzed = False\n",
    "\n",
    "# Specify what dataset to test on\n",
    "# dataset = 'circle_pairs_equal_l_red_blue/'\n",
    "# dataset = 'new_circles/'\n",
    "# dataset = '../Colorization_GAN/circle_pairs/'\n",
    "# test_dataset = 'lsun/test/'\n",
    "test_dataset = 'places2/test/'\n",
    "# train_dataset = 'places2/train/subdir/'\n",
    "# test_dataset = 'places2/10_imgs_per_cat/'\n",
    "# dataset = 'William_Henry_Jackson/WHJ_Resized_Square/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find where model is located\n",
    "saved_GAN_location = \"Output/\" + trained_model_name + \"/GAN_Weights_Epoch_100.h5\"\n",
    "saved_D_location = \"Output/\" + trained_model_name + \"/Discriminator_Weights_Epoch_100.h5\"\n",
    "\n",
    "# Create folder to store output\n",
    "generic_output_folder = \"Test_Output/\"\n",
    "\n",
    "if overwrite_save_dir is None:\n",
    "    new_output_folder = trained_model_name + \"/\"\n",
    "    save_path = generic_output_folder + new_output_folder\n",
    "else:\n",
    "    save_path = generic_output_folder + overwrite_save_dir + \"/\"\n",
    "    \n",
    "if random_select_gt_or_colorzed:\n",
    "    save_path += \"random_colorized_or_ground_truth/\"\n",
    "else:\n",
    "    save_path += \"all_predictions/\"\n",
    "\n",
    "# Ensure output can save in desired location\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831e3dbbffc4484685fab8cdbdab67ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading Images...', style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================================\n",
    "# COULD NOT HANDLE LARGE TRAINING SET\n",
    "# ===================================\n",
    "\n",
    "# Get training images\n",
    "# Load dataset, convert to LAB, normalize to range [-1, 1]\n",
    "# data = load_images(dataset + 'test', num_test_imgs)\n",
    "# data = load_images(dataset + \"/test/\", num_test_imgs)\n",
    "data = load_images(test_dataset, num_test_imgs)\n",
    "\n",
    "\n",
    "\n",
    "# Only want l channel\n",
    "l_channel_imgs, ab_channel_imgs = data['l'], data['ab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Disciminator Model:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 256, 256, 2)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256, 256, 3)  0           input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 64) 3136        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 128, 128, 64) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 128)  131200      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 64, 64, 128)  0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 128)  512         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 256)  524544      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 32, 32, 256)  0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 256)  1024        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 512)  2097664     batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 16, 16, 512)  0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 512)  2048        leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pred_valid (Conv2D)             (None, 16, 16, 1)    8193        batch_normalization_3[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 2,768,321\n",
      "Trainable params: 2,766,529\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/drew/anaconda3/envs/tf-gpu-1.14/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "Generator Model:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 65536)        6619136     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 256, 256, 1)  0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 256, 256, 2)  0           input_4[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 64) 2112        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 128, 128, 64) 0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 128)  131200      leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 64, 64, 128)  0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 128)  512         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 256)  524544      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 32, 32, 256)  0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 256)  1024        leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 512)  2097664     batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 16, 16, 512)  0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 512)  2048        leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 512)    4194816     batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 8, 8, 512)    0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 8, 8, 512)    2048        leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 4, 4, 512)    4194816     batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 4, 4, 512)    0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 4, 4, 512)    2048        leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 2, 2, 512)    4194816     batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 2, 2, 512)    0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 2, 2, 512)    2048        leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 4, 4, 512)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 4, 4, 512)    4194816     up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 4, 4, 512)    2048        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 4, 4, 1024)   0           batch_normalization_10[0][0]     \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 8, 8, 1024)   0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 512)    8389120     up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 8, 8, 512)    2048        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 8, 8, 1024)   0           batch_normalization_11[0][0]     \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 16, 16, 1024) 0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 512)  8389120     up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 512)  2048        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 16, 16, 1024) 0           batch_normalization_12[0][0]     \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 32, 32, 1024) 0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 256)  4194560     up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 256)  1024        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 512)  0           batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 64, 64, 512)  0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, 64, 128)  1048704     up_sampling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64, 64, 256)  0           batch_normalization_14[0][0]     \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 128, 128, 256 0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 64) 262208      up_sampling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 128, 128, 64) 256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 128, 128, 128 0           batch_normalization_15[0][0]     \n",
      "                                                                 leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 256, 256, 128 0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pred_ab (Conv2D)                (None, 256, 256, 2)  4098        up_sampling2d_7[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 48,459,394\n",
      "Trainable params: 48,450,562\n",
      "Non-trainable params: 8,832\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake_ab: (?, 256, 256, 2)\n",
      "gan_condition_in: (?, 256, 256, 1)\n",
      "true: (?, 256, 256, 1, 2)\n",
      "pred: (?, 256, 256, 1, 2)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gen_model (Model)               (None, 256, 256, 2)  48459394    input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "discrim_model (Model)           (None, 16, 16, 1)    2768321     gen_model[1][0]                  \n",
      "                                                                 input_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 51,227,715\n",
      "Trainable params: 48,450,562\n",
      "Non-trainable params: 2,777,153\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# GAN creation\n",
    "H = W = 256\n",
    "\n",
    "# Discriminator loss - MSE seems to produce better results\n",
    "#discrim_loss = 'binary_crossentropy'\n",
    "discrim_loss = 'mse'\n",
    "\n",
    "# 1. Discriminator\n",
    "# Calculate output shape of D (PatchGAN)\n",
    "patch = H // 2**4 # Input size gets cut in half 4 times\n",
    "discriminator = get_discriminator(H, W, k)\n",
    "discriminator.name = 'discrim_model' # Need a name for the loss dictionary below\n",
    "discriminator.compile(optimizer=Adam(2e-4, 0.5), loss=discrim_loss, metrics=['accuracy'])\n",
    "discriminator.trainable = False # For the combined model we will only train the generator\n",
    "print(\"\\n\")\n",
    "\n",
    "# 2. Generator\n",
    "generator = get_generator(H, W, k)\n",
    "generator.name = 'gen_model' # Need a name for the loss dictionary below\n",
    "\n",
    "# 3. GAN\n",
    "gan_noise_in = Input(shape=(100,))\n",
    "gan_condition_in = Input(shape=(H, W, 1))\n",
    "\n",
    "# By conditioning on L generate a fake version of AB\n",
    "fake_AB = generator([gan_noise_in, gan_condition_in])\n",
    "\n",
    "# Discriminator determines validity of AB images / L pairs\n",
    "print(\"fake_ab:\", fake_AB.shape)\n",
    "\n",
    "print(\"gan_condition_in:\", gan_condition_in.shape)\n",
    "\n",
    "valid = discriminator([fake_AB, gan_condition_in])\n",
    "\n",
    "losses = {'gen_model': min_k_diff, # used to be 'gen_loss'\n",
    "          'discrim_model': discrim_loss}\n",
    "loss_weights = {'gen_model': 100.0, 'discrim_model': 1.0}\n",
    "\n",
    "gan = Model(inputs=[gan_noise_in, gan_condition_in], outputs=[fake_AB, valid])\n",
    "gan.compile(optimizer=Adam(2e-4, 0.5), loss=losses, loss_weights=loss_weights)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output/places2_final_k_1_with_noise/GAN_Weights_Epoch_100.h5\n",
      "Output/places2_final_k_1_with_noise/Discriminator_Weights_Epoch_100.h5\n"
     ]
    }
   ],
   "source": [
    "# saved_GAN_location = \"Output/places2_final_k_1_with_noise/GAN_Weights_Epoch_100.h5\"\n",
    "print(saved_GAN_location)\n",
    "print(saved_D_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load the weights\n",
    "discriminator.load_weights(saved_D_location)\n",
    "gan.load_weights(saved_GAN_location)\n",
    "\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for 100 images complete!\n"
     ]
    }
   ],
   "source": [
    "noise = generate_noise(len(l_channel_imgs), 100)\n",
    "\n",
    "# colorized_predictions is [num_test_imgs, k]\n",
    "colorized_predictions = generator.predict([noise, l_channel_imgs])\n",
    "\n",
    "print(\"Predictions for\", len(colorized_predictions), \"images complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test_Output/LSUN_on_places2/all_predictions/'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if random_select_gt_or_colorzed:\n",
    "    # Make models so we can determine best prediction given discrim value\n",
    "    print(\"Building models...\")\n",
    "    models = make_discrim_models()\n",
    "    \n",
    "    # Get discrim values and reshape\n",
    "    print(\"Making discrim predictions...\")\n",
    "    discrim_values = discriminator.predict([colorized_predictions, l_channel_imgs])\n",
    "    print(\"Predictions complete!\\n\")\n",
    "    n = discrim_values.shape\n",
    "    reshaped_predictions_discrim = discrim_values.reshape((n[0], n[1]*n[2], n[3]))\n",
    "\n",
    "    # Get probabilities for each k\n",
    "    print(\"Testing each k...\")\n",
    "    probs = []\n",
    "    for i in range(k):\n",
    "        k_discrim_value = reshaped_predictions_discrim[:,:,i]\n",
    "        prob = models[i].predict_proba(k_discrim_value)\n",
    "        probs.append(prob[:,1])\n",
    "        print(\"k =\", i, \"complete!\")\n",
    "        \n",
    "    # Store best prediction index\n",
    "    best_prediction_indices = np.argmax(np.array(probs), 0)\n",
    "    \n",
    "    print('best_prediction_indices:', best_prediction_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging, deprocessing, converting to RGB, and saving images\n",
      "-- 25 completed\n",
      "-- 50 completed\n",
      "-- 75 completed\n",
      "-- 100 completed\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging, deprocessing, converting to RGB, and saving images\")\n",
    "\n",
    "num_ground_truth_selected = 0\n",
    "original_img_names = list_image_files(test_dataset)\n",
    "\n",
    "# Loop through images that were colorized\n",
    "for i, img in enumerate(colorized_predictions):\n",
    "    if preserve_img_names:\n",
    "        img_name = original_img_names[i]\n",
    "        filename = img_name.split(\"/\")[-1].split(\".\")[0]\n",
    "    else:\n",
    "        filename = str(i+1).zfill(len(str(num_test_imgs)))\n",
    "\n",
    "    # For each img, use either ground truth or random colorized prediction\n",
    "    if random_select_gt_or_colorzed:\n",
    "        # Determine whether to use ground truth or prediction\n",
    "        use_ground_truth = random.choice([True, False])\n",
    "        \n",
    "        # Either use ground truth or best prediction\n",
    "        if use_ground_truth:\n",
    "            num_ground_truth_selected += 1\n",
    "            filename += \"_Ground_Truth.png\"\n",
    "            save_rgb_img(l_channel_imgs[i], ab_channel_imgs[i], filename)\n",
    "        else:\n",
    "            prediction_i = best_prediction_indices[i]\n",
    "            filename += \"_Colorized_.png\"\n",
    "            prediction = img[:,:,2*prediction_i:2*prediction_i+2]\n",
    "            save_rgb_img(l_channel_imgs[i], prediction, filename)\n",
    "    else:\n",
    "        # Save original\n",
    "        save_rgb_img(l_channel_imgs[i], ab_channel_imgs[i], filename + \" original.png\")\n",
    "        # Save grayscale\n",
    "        gray = np.zeros((256, 256, 2))\n",
    "        save_rgb_img(l_channel_imgs[i], gray, filename + \" grayscale.png\")\n",
    "        \n",
    "        # Loop through predictions\n",
    "        for j in range(k):\n",
    "            prediction = img[:,:,2*j:2*j+2]\n",
    "            save_rgb_img(l_channel_imgs[i], prediction, filename + \"-\" + str(j+1) + \".png\")\n",
    "            \n",
    "    if (i + 1) % 25 == 0:\n",
    "        print(\"--\", i+1, \"completed\")\n",
    "        \n",
    "print(\"DONE!\")\n",
    "\n",
    "if random_select_gt_or_colorzed:\n",
    "    print(\"\\nBreakdown of Ground Truth vs. Colorized Selected:\")\n",
    "    print(\"Ground Truth:\", num_ground_truth_selected, \"--\", str(100 * num_ground_truth_selected / len(l_channel_imgs)) + \"%\")\n",
    "    print(\"Colorized:\", len(l_channel_imgs) - num_ground_truth_selected, \"--\", str(100 * (len(l_channel_imgs) - num_ground_truth_selected) / len(l_channel_imgs)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Troubleshooting discriminator values\n",
    "\n",
    "# for i, img in enumerate(discrim_values):\n",
    "#     print('=====', i, '=====')\n",
    "#     for j in range(k):\n",
    "#         print(j, sum(img[:,:,j]))\n",
    "# #         guess = colorized_predictions[i][:,:,2*j:2*j+2].astype(np.float64)\n",
    "# # #         prediction = cv2.merge((np.expand_dims(l_channel_imgs[i], -1), colorized_predictions[i][:,:,2*i:2*i+2]))\n",
    "# #         print(guess.shape)\n",
    "# #         merged = cv2.merge((l_channel_imgs[i], guess))\n",
    "# #         plt.imshow(cv2.cvtColor(deprocess_image(merged), cv2.COLOR_LAB2RGB))\n",
    "# #         plt.show()\n",
    "#     print(\"MAX:\",np.argmax(sum(img, axis=(0,1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each k has its own discrim model\n",
    "\n",
    "# pred = colorized_predictions[:1, :, :, :2]\n",
    "# print(pred.shape)\n",
    "\n",
    "# disp = deprocess_image(cv2.merge((l_channel_imgs[0], ab_channel_imgs[0])))\n",
    "# disp = cv2.cvtColor(disp, cv2.COLOR_LAB2RGB)\n",
    "# plt.figure()\n",
    "# plt.imshow(disp)\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    " \n",
    "# disp = deprocess_image(cv2.merge((l_channel_imgs[0], pred[0].astype(np.float64))))\n",
    "# disp = cv2.cvtColor(disp, cv2.COLOR_LAB2RGB)\n",
    "# plt.figure()\n",
    "# plt.imshow(disp)\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    " \n",
    "# three_of_same = np.zeros((1, 256, 256, 6))\n",
    "# three_of_same[:, :, :, :2] += pred\n",
    "# three_of_same[:, :, :, 2:4] += pred\n",
    "# three_of_same[:, :, :, 4:] += pred\n",
    "\n",
    "\n",
    "# discrim_score = discriminator.predict([three_of_same, l_channel_imgs[:1]])\n",
    "# for x in range(3):\n",
    "#     plt.figure()\n",
    "#     plt.imshow(discrim_score[0, :, :, x])\n",
    "#     plt.colorbar()\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
